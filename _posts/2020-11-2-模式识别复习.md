---
title: 模式识别复习
---

第三章

- 鉴别函数
- 两类别facial判别，怎么判别，分类准则
- 感知器的准则函数、梯度下降算法
- 最小方差准则解析解
- SVM：线性可分svm及其推导，支持向量
- 松弛方程C的意义（**）

第二章

- 各种距离函数，马氏。。。

- 角度相似度怎么算
- 总离差阵等于。。。。的证明
- C均值聚类，准则函数，聚类类别怎么挑选
- 降维，多维放缩，PCA，核化线性降维PCA怎么推导的

第一章

两个证明

学习方法的三要素，什么是训练误差，测试误差，区别，判断改善模型过拟合

roc曲线怎么画的

#### 第一章

- 模式:对事物(研究对象特征的描述(定量的或结构的描述)。表现形式包括特征矢量、符号串、图或者关系式等。
- 样本：一个具体的研究对象
- 模式识别：确定一个样本的类别属性的过程，即把某一个样本归属于多个类型中的某个类型。

- 随机矢量的描述：

①协方差矩阵：随机矢量X的自协方差矩阵表征各分量围绕其均值的散布情况及各分量间的相关关系。

what is 协方差？

用于衡量两个变量的总体误差

![image-20210424092122127](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424092129.png)

②随机矢量的正态分布

![image-20210424092623700](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424092623.png)

##### 证明题 **

###### 一

试证明，对于正态分布，不相关与独立是等价的

###### 二

多元随机矢量的线性变化仍为多元正态随机矢量

- 学习方法的三要素

方法=模型+策略+算法

① 模型

当假设空间F为决策函数的集合时，F实质上为参数向量决定的函数族<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424170023.png" alt="image-20210424170023396" style="zoom:67%;" />

当假设空间F为条件概率的集合，F实质上是参数向量决定的条件概率分布族<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424170042.png" alt="image-20210424170042333" style="zoom:67%;" />

② 策略

- 损失函数：一次预测的好坏

0-1损失函数

平方损失函数

绝对值损失函数

对数损失函数

- 风险函数：平均意义下模型预测的好坏

风险函数等于损失函数的期望

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424190410.png" alt="image-20210424190410171" style="zoom:67%;" />

当样本容量很小时，经验风险最小化学习的效果未必很好，会产生过拟合

结构风险最小化，为防止过拟合提出的策略，加入正则化项或者罚项

③ 算法

求最优模型就是求解最优化问题：

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424191104.png" alt="image-20210424191104867" style="zoom:67%;" />

- 训练误差和测试误差

区别：训练误差是模型在训练集上的误差，泛化误差是在新样本上的误差，我们更希望得到泛化误差小的模型。测试误差是模型在测试集上的误差。训练误差的降低不一定意味着泛化误差的降低，机器学习既需要降低训练误差，又需要降低泛化误差。训练误差小，测试误差不一定小。

泛化误差：学习方法的泛化能力是指该方法学习到的模型对未知数据的预测能力。

- 欠拟合和过拟合（判断以及如何改善）

欠拟合：模型训练误差很大，测试集的测试也很大

过拟合，模型训练误差很小，但是测试误差很大，也即泛化能力较弱。

改善：

针对欠拟合

①：添加新特征

②： 模型优化，提高模型复杂度

③： 减少正则项权重

针对过拟合

①： 由于模型训练了包含噪音在内的所有特征，导致模型过拟合，通过**获取更多的训练样本**，可以衰减噪音权重。

②： 减少特征数目

如使用主成分分析，保留特征变量的重要差异

③： 增加正则项权重

- 训练集：用于模型训练
- 验证集：用于模型选择
- 测试集：用于模型泛化误差的近似

###### 训练集和测试集的产生

- 留出法
- 交叉验证法
- 自助法

##### ROC和AUC曲线

ROC曲线和AUC常被用来评价一个二值分类器的优劣

![image-20210424203951693](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424203951.png)

ROC：
横坐标为假阳性率=FP/N，预测为正但是实际为负的样本占所有负例样本的比例。
纵坐标为真阳性率=TP/P，预测为正实际也为正的样本占所有正例样本的比例。
ROC曲线越接近左上角，分类器的性能越好

![image-20210424205457242](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424205457.png)

第一个点（0，0.1）的意思：将0.9设为阈值，则
![image-20210424211449597](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424211449.png)

样本三：

![image-20210424211928129](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424211928.png)

AUC:是ROC曲线下的面积，是一个数值，沿着ROC横轴做积分。，AUC值越大，分类算法越好

第二章 聚类分析

###### 距离测度

- 欧氏距离：

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423184550.png" alt="image-20210423184542901" style="zoom:50%;" />

- 绝对值距离（曼哈顿距离）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423185735.png" alt="image-20210423185735250" style="zoom:50%;" />

- 马氏距离

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423194430.png" alt="image-20210423194430236" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423194554.png" alt="image-20210423194554249" style="zoom:67%;" />

多元正态分布：

![image-20210423192225544](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423192232.png)



![image-20210423192904376](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423192904.png)

 

##### 2 角度相似度怎么算

以两矢量的方向是否相近作为考虑的基础

- 角度相似系数（夹角余弦）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423195501.png" alt="image-20210423195501282" style="zoom:50%;" />

角度相似系数应用

###### 计算文本相似度

- 句子A：这只皮靴号码大了。那只号码合适
- 句子B：这只皮靴号码不小，那只更适合

基本思路：词频统计

① 分词

句子A：这只/皮靴/号码/大了。那只/号码/合适

句子B：这只/皮靴/号码/不/小，那只/更/合适

② 列出所有的词

这只、皮靴、号码、大了、不、小、那只、更、合适

③ 计算词频

句子A：这只1，皮靴1，号码2，大了1。那只1，合适1，不0，小0，更0 

句子B：这只1，皮靴1，号码1，大了0。那只1，合适1，不1，小1，更1

句子A：(1，1，2，1，1，1，0，0，0) 

句子B：(1，1，1，0，1，1，1，1，1)

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423202118.png" alt="image-20210423202118632" style="zoom: 67%;" />

推荐系统 *

###### 匹配测度

- Tanimoto测度

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423203402.png" alt="image-20210423203402181" style="zoom:50%;" />

Tanimoto测度等于共同具有的特征数目与分别具有的特征种类总数之比。这里只考虑（1-1）匹配而不考虑（0-0）匹配。

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423203825.png" alt="image-20210423203825881" style="zoom:50%;" />



##### 3 类的定义与类间距离

- 类间距测度方法

① 最近距离法

② 最远距离法

③中心距离法

④ 重心距离法

⑤ 平均距离法

⑥ 离差平方和法

- 聚类的准则函数

判断分类好坏的一般**标准**：

**类内距离小，类间距离大**

① 类内距离准则

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423211755.png" alt="image-20210423211755085" style="zoom:50%;" />

我们的目标是使J~w~ 取最小，即J~w~ ==>min,这种方法也称为**误差平方和准则**

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423221728.png" alt="image-20210423221728600" style="zoom:67%;" />

##### 总离差阵等于类内离差阵与类间离差阵之和（**证明题）

![微信图片_20210425194401](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425194414.jpg)

式中m向量为所有待分类模式的均值矢量，（n~j~ /N表示w~j~ 类先验概率的估计——频率）。

![](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425194609.png)

##### 动态聚类法（***）

步骤：

① 建立初始聚类中心，进行初始聚类

② 计算模式和类的距离，调整模式的类别

③ 计算各聚类的参数，删除、合并或分裂一些聚类

④ 从初始聚类开始，运用迭代算法动态地改变模式的类别和聚类的中心使准则函数取得极值或设定的参数达到设计要求时停止。

![image-20210425201714992](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425201715.png)

思想：该方法取定C个类别和选取C个初始聚类中心，按最小距离原则将各模式分配到C类中的某一类，之后不断地计算类心和调整各模式的类别，最终使各模式到其判属类别中心的距离平方之和最小。

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203003.png" alt="image-20210425203003767" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203027.png" alt="image-20210425203027279" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203049.png" alt="image-20210425203049747" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203235.png" alt="image-20210425203235658" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203301.png" alt="image-20210425203301313" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203335.png" alt="image-20210425203335409" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203446.png" alt="image-20210425203446067" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203506.png" alt="image-20210425203506315" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203534.png" alt="image-20210425203534608" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203614.png" alt="image-20210425203614639" style="zoom:50%;" />

##### 核化线性降维PCA推导

- 核技巧

基本思想：通过一个核函数（核函数）将输入空间的向量内积对应一个高维特征空间的向量内积，在高维特征空间中把数据投影到由W确定的超平面上。





#### 第三章

