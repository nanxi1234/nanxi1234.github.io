---
title: 模式识别复习
---

第三章

- 鉴别函数
- 两类别Fisher判别，怎么判别，分类准则
- 感知器的准则函数、梯度下降算法
- 最小方差准则解析解
- SVM：线性可分SVM及其推导，支持向量
- 松弛方程C的意义（**）

第二章

- 各种距离函数，马氏。。。

- 角度相似度怎么算
- 总离差阵等于。。。。的证明
- C均值聚类，准则函数，聚类类别怎么挑选
- 降维，多维放缩，PCA，核化线性降维PCA怎么推导的

第一章

两个证明

学习方法的三要素，什么是训练误差，测试误差，区别，判断改善模型过拟合

roc曲线怎么画的

#### 第一章

- 模式:对事物(研究对象特征的描述(定量的或结构的描述)。表现形式包括特征矢量、符号串、图或者关系式等。
- 样本：一个具体的研究对象
- 模式识别：确定一个样本的类别属性的过程，即把某一个样本归属于多个类型中的某个类型。

- 随机矢量的描述：

①协方差矩阵：随机矢量X的自协方差矩阵表征各分量围绕其均值的散布情况及各分量间的相关关系。

what is 协方差？

用于衡量两个变量的总体误差

![image-20210424092122127](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424092129.png)

②随机矢量的正态分布

![image-20210424092623700](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424092623.png)

##### 证明题 **

###### 一

试证明，对于正态分布，不相关与独立是等价的

###### 二

多元随机矢量的线性变化仍为多元正态随机矢量

- 学习方法的三要素（***）

方法=模型+策略+算法

① 模型

当假设空间F为决策函数的集合时，F实质上为参数向量决定的函数族<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424170023.png" alt="image-20210424170023396" style="zoom:67%;" />

当假设空间F为条件概率的集合，F实质上是参数向量决定的条件概率分布族<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424170042.png" alt="image-20210424170042333" style="zoom:67%;" />

② 策略

- 损失函数：一次预测的好坏

0-1损失函数

平方损失函数

绝对值损失函数

对数损失函数

- 风险函数：平均意义下模型预测的好坏

风险函数等于损失函数的期望

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424190410.png" alt="image-20210424190410171" style="zoom:67%;" />

当样本容量很小时，经验风险最小化学习的效果未必很好，会产生过拟合

结构风险最小化，为防止过拟合提出的策略，加入正则化项或者罚项

③ 算法

求最优模型就是求解最优化问题：

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424191104.png" alt="image-20210424191104867" style="zoom:67%;" />

- 训练误差和测试误差

区别：训练误差是模型在训练集上的误差，**泛化误差是在新样本上的误差**，我们更希望得到泛化误差小的模型。**测试误差是模型在测试集上的误差。**训练误差的降低不一定意味着泛化误差的降低，机器学习既需要降低训练误差，又需要降低泛化误差。训练误差小，测试误差不一定小。

泛化误差：学习方法的**泛化能力**是指该**方法学习到的模型对未知数据的预测能力**。

- **欠拟合和过拟合（判断以及如何改善）**

欠拟合：模型训练误差很大，测试集的测试误差也很大

过拟合，模型训练误差很小，但是测试误差很大，也即泛化能力较弱。

改善：

针对欠拟合：

①：添加新特征

②： 模型优化，提高模型复杂度

③： 减少正则项权重

针对过拟合：

①： 由于模型训练了包含噪音在内的所有特征，导致模型过拟合，通过**获取更多的训练样本**，可以衰减噪音权重。

②： 减少特征数目

如使用主成分分析，保留特征变量的重要差异

③： 增加正则项权重

- 训练集：用于模型训练
- 验证集：用于模型选择
- 测试集：用于模型泛化误差的近似

###### 训练集和测试集的产生

- 留出法
- 交叉验证法
- 自助法

##### ROC和AUC曲线

ROC曲线和AUC常被用来评价一个二值分类器的优劣

![image-20210424203951693](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424203951.png)

ROC：
横坐标为假阳性率=FP/N，预测为正但是实际为负的样本占所有负例样本的比例。
纵坐标为真阳性率=TP/P，预测为正实际也为正的样本占所有正例样本的比例。
ROC曲线越接近左上角，分类器的性能越好

![image-20210424205457242](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424205457.png)

第一个点（0，0.1）的意思：将0.9设为阈值，则
![image-20210424211449597](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424211449.png)

样本三：

![image-20210424211928129](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210424211928.png)

AUC:是ROC曲线下的面积，是一个数值，沿着ROC横轴做积分。，AUC值越大，分类算法越好

第二章 聚类分析

###### 距离测度

- 欧氏距离：

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423184550.png" alt="image-20210423184542901" style="zoom:50%;" />

- 绝对值距离（曼哈顿距离）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423185735.png" alt="image-20210423185735250" style="zoom:50%;" />

- 马氏距离

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423194430.png" alt="image-20210423194430236" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423194554.png" alt="image-20210423194554249" style="zoom:67%;" />

多元正态分布：

![image-20210423192225544](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423192232.png)



![image-20210423192904376](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423192904.png)

 

##### 2 角度相似度怎么算

以两矢量的方向是否相近作为考虑的基础

- 角度相似系数（夹角余弦）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423195501.png" alt="image-20210423195501282" style="zoom:50%;" />

角度相似系数应用

###### 计算文本相似度

- 句子A：这只皮靴号码大了。那只号码合适
- 句子B：这只皮靴号码不小，那只更适合

基本思路：词频统计

① 分词

句子A：这只/皮靴/号码/大了。那只/号码/合适

句子B：这只/皮靴/号码/不/小，那只/更/合适

② 列出所有的词

这只、皮靴、号码、大了、不、小、那只、更、合适

③ 计算词频

句子A：这只1，皮靴1，号码2，大了1。那只1，合适1，不0，小0，更0 

句子B：这只1，皮靴1，号码1，大了0。那只1，合适1，不1，小1，更1

句子A：(1，1，2，1，1，1，0，0，0) 

句子B：(1，1，1，0，1，1，1，1，1)

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423202118.png" alt="image-20210423202118632" style="zoom: 67%;" />

推荐系统 *

###### 匹配测度

- Tanimoto测度

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423203402.png" alt="image-20210423203402181" style="zoom:50%;" />

Tanimoto测度等于共同具有的特征数目与分别具有的特征种类总数之比。这里只考虑（1-1）匹配而不考虑（0-0）匹配。

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423203825.png" alt="image-20210423203825881" style="zoom:50%;" />



##### 3 类的定义与类间距离

- 类间距测度方法

① 最近距离法

② 最远距离法

③中心距离法

④ 重心距离法

⑤ 平均距离法

⑥ 离差平方和法

- 聚类的准则函数

判断分类好坏的一般**标准**：

**类内距离小，类间距离大**

① 类内距离准则

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423211755.png" alt="image-20210423211755085" style="zoom:50%;" />

我们的目标是使J~w~ 取最小，即J~w~ ==>min,这种方法也称为**误差平方和准则**

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210423221728.png" alt="image-20210423221728600" style="zoom:67%;" />

##### 总离差阵等于类内离差阵与类间离差阵之和（**证明题）

![微信图片_20210425194401](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425194414.jpg)

式中m向量为所有待分类模式的均值矢量，（n~j~ /N表示w~j~ 类先验概率的估计——频率）。

![](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425194609.png)

##### 动态聚类法（***）

步骤：

① 建立初始聚类中心，进行初始聚类

② 计算模式和类的距离，调整模式的类别

③ 计算各聚类的参数，删除、合并或分裂一些聚类

④ 从初始聚类开始，运用迭代算法动态地改变模式的类别和聚类的中心使准则函数取得极值或设定的参数达到设计要求时停止。

![image-20210425201714992](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425201715.png)

思想：该方法取定C个类别和选取C个初始聚类中心，按**最小距离原则**将各模式分配到C类中的某一类，之后不断地计算类心和调整各模式的类别，最终**使各模式到其判属类别中心的距离平方之和最小。**

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203003.png" alt="image-20210425203003767" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203027.png" alt="image-20210425203027279" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203049.png" alt="image-20210425203049747" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203235.png" alt="image-20210425203235658" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203301.png" alt="image-20210425203301313" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203335.png" alt="image-20210425203335409" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203446.png" alt="image-20210425203446067" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203506.png" alt="image-20210425203506315" style="zoom:50%;" />









<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203534.png" alt="image-20210425203534608" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210425203614.png" alt="image-20210425203614639" style="zoom:50%;" />

C-均值法中隐含地运用了准则函数

![image-20210427150132222](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427150132.png)

随着K的增加，j^(k)^ 趋于变小。

##### 核化线性降维PCA推导

PCA：使用一个超平面对所有样本进行恰当的表达

这个超平面具有以下性质：

- 最近重构性：样本点到这个超平面的距离足够近（尽量少的信息损失）
- 最大可分性：样本点到这个超平面的投影，尽量分开

将一组N维向量降为K维，其目标是选择K个单位正交基。使得原始数据变换到这组基后，各字段两两协方差为0，而字段的方差尽可能大（在正交的约束下，取最大的K个方差）。

本质：把方差最大的方向作为主要特征，并且在各个方向上将数据离相关，使它们在不同的正交方向上没有相关性。

- 核技巧

基本思想：通过一个核函数（核函数）将输入空间的向量内积对应一个高维特征空间的向量内积，在高维特征空间中把数据投影到由W确定的超平面上。

为什么需要它？

PCA对于高阶相关性没有办法，通过核函数将非线性转化为线性。从而实现处理线性不可分的数据集。

基本思路：对于输入空间中的矩阵X，先用一个非线性映射把X中的所有样本映射到一个高维或者无穷维的空间，然后在这个高维空间进行PCA降维

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428104041.png" alt="image-20210428104041610" style="zoom:67%;" />





#### 第三章

##### 两类别Fisher判别，怎么判别，分类准则（***）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210426163805.png" alt="image-20210426163758780" style="zoom:50%;" />

- 两类别Fish判别，怎么判别，分类准则

Fisher准则函数的基本思路是**向量W的方向选择应能使两类样本投影的均值之差尽可能大些，而使类内样本的离散程度尽可能小。**

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427194811.png" alt="image-20210427194811389" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210426165543.png" alt="image-20210426165543253" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210426165615.png" alt="image-20210426165615128" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210426170859.png" alt="image-20210426170859574" style="zoom:50%;" />

下面求出**u**并把它带入到准则函数J~F~ 中求出J~F~ 。

​    Fisher判别函数：变换之后的模式是一维的，因此判别界面实际上是各类模式所在轴上的一个点，因此可以根据训练模式确定阈值y~t~ 

​            <img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427202312.png" alt="image-20210427202312239" style="zoom:67%;" />

判别阈值的选择：取两个类心在**u** 方向上轴的投影连线的中点作为阈值：

​            <img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427202602.png" alt="image-20210427202602898" style="zoom:67%;" />



##### 感知器算法

<img src="C:\Users\zhaohuizhang\AppData\Roaming\Typora\typora-user-images\image-20210426183111780.png" alt="image-20210426183111780" style="zoom:50%;" />

- 问题必须是线性可分的，否则陷入死循环
- 用训练模式检验初始的或迭代中的增广权矢量W的合理性，当不合理时，对其进行校正。
- 校正算法实际上是最优化技术中的梯度下降法。

##### 一次准则函数

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210426194740.png" alt="image-20210426194740591" style="zoom:50%;" />

- 当ρ~k~ 为常数时，梯度下降法的迭代公式与感知器训练算法是一致的。
- 当ρ~k~ 为常数时，这种梯度下降法也称为固定增量法，若ρ~k~ 取得较小，收敛速度较慢，取得较大收敛速度加快，但是搜索到接近极值点时，可能产生过调引起振荡。
- 改进的方法是让ρ~k~ 随k变化，称之为可变增量法。

##### 松弛方程C的意义（***简答）

对于不是严格线性可分的，SVM可以求次优解，解决方法是：允许SVM在少量样本上出错，具体的做法就是在优化目标函数上新增一个对这些点的惩罚项

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427111659.png" alt="image-20210427111659334"  />

则优化目标变为：

![image-20210427111802075](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427111802.png)

C>0称为惩罚函数，C越小对误分类惩罚越小，越容易欠拟合，越大时对误分类惩罚越大，当C取正无穷时就变成了硬间隔优化。

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427160924.png" alt="image-20210427160924711" style="zoom: 50%;" />四种情况

##### 多维缩放（MDS）算法（推导***）

基本思想：保持样本在原空间和低维空间的距离不变

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428094436.png" alt="image-20210428094436042" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428101156.png" alt="image-20210428101156765" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428101229.png" alt="image-20210428101228942" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428101453.png" alt="image-20210428101453122" style="zoom:67%;" />

##### 给你一个数据集，如何划分训练集和验证集？交叉验证是怎么回事？（简答**）



- 留出法

  D=S∪T      S∩T=空集

  即把数据集划分为两个互斥的集合，其中一个集合作为训练集S，留下的集合作为测试集T。

- 交叉验证法

​       D—>K个大小相等的互斥子集

​       D=D~1~ ∪D~2~ ∪…∪D~k~ ，D~i~ ∩D~j~ =空集

​        K-1个子集并集为训练集，1个测试集，重复这个步骤N次

​        优点在于每个数据都单独做过测试集的同时，用了n-1个数据训练模型，几乎用到了所有数据，缺点是计算量过大。

- 自助法

DD进行采样产生D′D′：每次随机从DD中挑选一个样本，将其拷贝一份放入D′D′中，保持DD不变，重复以上过程mm次。显然，DD中有部分样本会多次出现在D′D′中，而另一部分不会出现。样本在mm次采样中的始终不被采到的概率为

![image-20210429105830528](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210429105837.png)

##### 1.分类的基本原理

不同模式对应特征点在不同的区域中散布，运用已知类别的训练样本进行学习，产生若干个代数界面d(**x**)=0，将特征空间划分为一些互不重叠的子区域。

分类方法的基本技术思路

- 利用训练样本求出分类器/判别函数
- 利用判别函数对未知类别样本分类

##### 判别函数的意义（三个证明）***

N维空间X^n^ 中，两类问题的线性判别界面方程为：

​          d(**x**)=**W~0~ ‘** **X**+W~n+1~ 

此方程表示一超平面Π，有以下三个性质：

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427163116.png" alt="image-20210427163116755" style="zoom:67%;" />

分别证明

 （1）

​       设**x~1~ ** **x~2~ ** 在判别界面中，故他们满足方程

​       带入做差有d(x~1~ —x~2~ )=<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427163411.png" alt="image-20210427163411478" style="zoom:67%;" />

​       又**x~1~ ** **x~2~ ** 在Π平面上，则1得证

（2）

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427163926.png" alt="image-20210427163926324" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427163944.png" alt="image-20210427163944211" style="zoom:67%;" />

（3)判别函数值的正负表示出特征点位于哪个半空间中

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427165616.png" alt="image-20210427165616276" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210427165642.png" alt="image-20210427165642642" style="zoom:67%;" />

##### 多类问题两分法（三种情况）

① <img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428085450.png" alt="image-20210428085442808" style="zoom:67%;" />

缺点：如果某个X使两个以上的判别函数d~i~ >0。则此模式X就无法做出确切的判决。

② 对C类中的任意两类w~i~ 和w~j~ 都分别建立一个判别函数，这个判别函数将属于w~i~ 的模式与属于w~j~ 的模式区分开。此函数对其他模式分类不提供信息，因此需要c（c-1）/2个这样的判别函数。

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428092210.png" alt="image-20210428092210614" style="zoom:67%;" />

③ 没有不确定区的两分法

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428093252.png" alt="image-20210428093252259" style="zoom:67%;" />

<img src="https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210428093734.png" alt="image-20210428093734744" style="zoom:67%;" />

