---
date: 2021-02-17
title: Java容器
author: 南夕
---

###### 简介

HashMap:根据键的hashcode值存储数据，具有很快的访问速度，但是遍历的顺序不确定,非线程安全（**因为在 put 元素的时候，如果触发扩容操作，也就是 rehash ，就会将原数组的内容重新 hash 到新的扩容数组中，但是在扩容这个过程中，其他线程也在进行 put 操作，如果这两个元素 hash 值相同，可能出现同时在同一数组下用链表表示，造成闭环，导致在get时会出现死循环，所以HashMap是线程不安全的**。），可用Collections.synchronizedMap方法使hashmap具有线程安全的能力，或者使用ConcurrentHashMap。

HashTable:线程安全

LinkedHashMap：是HashMap的一个子类，保存了插入的顺序，在使用Iterator遍历时，先得到的肯定是先插入的。

TreeMap:实现SortedMap接口，能够把它保存的记录按照键排序

以上四种Map类型，要求映射中的key是不可变对象，不可变对象是该对象在创建后它的哈希值不会被改变，如果对象的哈希值被改变，将无法定位到映射的位置。

##### 内部实现

  存储结构-字段

数组+链表+红黑树（java 1.8增加）

数据底层具体存储什么？这样存储的好处？

 

| threshold  |               所能容纳的key-value对极限               |
| :--------: | :---------------------------------------------------: |
| LoadFactor |                  负载因子，默认0.75                   |
|  modCount  | 记录HashMap内部结构发生变化的次数，用于迭代的快速失败 |
|    size    |                 实际存在的键值对数量                  |

length的初始值为16

  threshold = length * LoadFactor

负载因子0.75是对空间和时间效率的一个平衡选择（即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小），如果内存空间很多而对时间效率很高，可以降低负载因子的值，反之则可以增加。

为什么哈希桶数组的长度必须为2的n次方，主要是为了取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。

问题：拉链过长怎么办？

一旦拉链过长，势必会影响性能，当拉链过长时，采用红黑树保存节点。

什么是红黑树，讲一下？

红黑树本质上就是一颗二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质，使得红黑树相对平衡，从而保证了红黑树的查找\插入、删除的时间复杂度最坏为O（logn）。

如何做到的？

- 每个节点要么是红的，要么是黑的
- 根节点是黑的
- 每个叶节点是黑的
- 如果一个节点是红的，那么它的两个都是黑的
- 对于任一节点，其到叶节点尾端的每一条路径都包含相同数量的黑节点

###### 功能实现

确定哈希桶数组索引位置

哈希算法的目标：希望HashMap里的元素位置尽量分布均匀，使得每个位置上的元素数量尽可能的少

步骤：取key的hashcode值，高位运算、取模运算

```java
//方法一
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
//方法二
static int indexFor(int h,int length){//jdk1.7源码，jdk1.8没有这个方法，但实现原理是一样的
    return h & (length-1);
}
```

对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同。把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的，但是模运算的消耗比较大。

HashMap的解决方案：调用方法二来计算对象应该保存在table数组的哪个索引处。

- 为什么容量都是2的幂？（默认长度为16应该是经验值）

1. 方法二通过h & （table.length-1）来得到该对象的保存位，而HashMap底层数组的长度总是2的N次方，所以h & (table.length-1)与h%length是相同的，但是&比%具有更高的效率。

2. 碰撞概率会比较低，容量-1之后二进制数为全1，这样与运算就等于hash值的后面进行与运算的几位
3. 让HashMap扩容时可以方便的重新计算位置

为什么要采用高位运算？

hashcode的高16位异或低16位

可以在table的length比较小的时候，也能保证高低位都参与到Hash值的计算之中，减少低位相同时数据插入冲突的概率

###### hashmap的扩容机制

扩容就是重新计算容量，向HashMap对象中不停的添加对象，当装不下时，就需要扩大数组的长度，具体做法是用一个新的数组代替已有的容量小的数组

###### 解释一下为什么重写 equals 方法的时候还需要重写 hashCode 方法呢？

HashMap中get(key)一个元素会先比较hashcode是否相等，若相等再通过equals()比较其值，若不重写hashcode会导致相同的对象拥有不同的hashcode，导致判断出错，如果不重写equals会导致equals比较的是地址而不是值，相同值不同地址会被认为不相等导致判断出错。

###### HashMap线程不安全性表现有哪些？

- JDK1.7中并发环境会导致循环链表的问题

- JDK1.8由于put（）未上锁，并发环境下可能发生某个线程插入的数据被覆盖的问题

- ###### 线程安全的Map

  使用Collections.synchronizedMap(Map)创建线程安全的Map。

  实现原理：加synchronized（mutex）排它锁

  使用ConcurrentHashMap

  使用HashTable

###### 红黑树/链表

链表长度大于等于8时，会把链表转换成红黑树，不过在转换之前，会先去查看table数组的长度是否大于64，如果数组长度小于64，那么会优先选择对数组进行扩容，而不是把链表转换成红黑树。小于等于6个时，退回链表。为什么中间有差值？避免频繁发生链表转树，树转链表。

节点如何插入链表的？

JDK1.7使用头插法，JDK1.8改用尾插法，因为JDK1.7中的头插法在多线程环境下可能会造成循环链表的问题。JDK1.7中HashMap使用头插会改变链表上元素的顺序，在旧数组向新数组转移元素的过程中修改了链表中节点的引用关系，因此改用尾插法，扩容时就能保证链表元素原本的顺序，避免了链表成环的问题。

###### 快速失败

是一种安全机制，可以在多线程操作不安全的集合时，抛出异常。可以让HashMap在遍历时，如果有线程对集合进行了修改、删除、增加操作，会促发并发修改异常。

如何实现？

在遍历前保存一份modCount，在每次获取下一个要遍历的元素时会对比当前的modCount和保存的modCount是否相等。

##### ConcurrentHashMap

线程安全的HashMap

- JDK 1.7 与 1.8区别

1.8中去除了Segment+HashEntry+Unsafe的实现方式，改为Synchronized+CAS+Node+Unsafe的实现方式

###### JDK 1.7的实现

为了解决HashTable会锁住整个hash表的问题，提出了分段锁的解决方案，分段锁就是将一个大的hash表分解成若干份小的hash表，需要加锁时就针对小的hash表进行加锁，从而提升hash表的性能，JDK1.7中的ConcurrentHashMap引入了segment对象，将整个hash表分解成一个一个的Segment对象，每个Segment对象可以看成是一个细粒度的HashMap。

Segment对象继承了RetrantLock类，因此segment对象就变成了一把锁，这样就可以保证数据的安全。在segment对象中通过HashEntry数组来维护其内部的hash表，每个HashEntry就代表了map中的一个K-V，如果发生hash冲突时，在该位置就会形成链表。

![ConcurrentHashMap 1.7 存储结构](https://cdn.jsdelivr.net/gh/nanxi1234/nanxi1234.github.io/image/2021/20210923101346.png)

- ConcurrentHashMap如何解决HashMap put时扩容引起的不安全问题？

首先通过二次哈希减少哈希冲突的可能性，根据hash值以Unsafe调用方式，直接获取相应的Segment，最终将数据添加到容器中是由segment对象的put方法来完成。

由于segment对象本身就是一把锁，所以在新增数据时，其他线程不能操作这个Segment对象，这就保证了数据的安全性，扩容时，JDK 1.7中的ConcurrentHashMap扩容只是针对Segment对象中的HashEntry数组进行扩容，在rehash的过程中，其他线程无法对segment的hash表做操作。

优缺点：hash过程要比普通的HashMaP长，影响性能，在1.7版本中，定位到一个元素需要通过两次Hash计算，第一次计算定位到Segment，第二次计算定位到HashEntry（所在元素的头结点），但写操作的时候只对元素所在Segment进行加锁即可，不会影响到其它的Segment，ConcurrentHashMap提升了并发能力。

###### JDK 1.8 中的ConcurrentHashMap

在保证安全性上：1.8的ConcurrentHashMap放弃了JDK 1.7中的分段技术，而是采用了CAS+synchronized来保证并发安全性。

在存储结构上，JDK 1.8 中放弃了HashEntry结构而是采用了Node数组加链表（链表长度大于8时转成红黑树）的形式。

在ConcurrentHashMap根据双哈希之后的哈希值找到数组对应的下标位置，如果该位置未存放节点，则说明不存在hash冲突。使用CAS无锁的方式将数据添加到容器中。

如果该位置已有节点，则会判断容器是否正在被其他线程进行扩容，如果是，则放弃扩容操作。

如果哈希冲突，则进行链表操作或红黑树操作，在进行链表或者红黑树操作时，会用synchronized锁把头结点锁住，保证了同时只有一个线程修改链表，防止链表成环。

- 不允许key或value为null的原因

因为是多线程的，如果map.get(key)得到了null，不能判断到底映射的value是null，还是因为没有找到相应的key而为空。

当我们首先从map中get某个key，由于map中这个key不存在，那么会返回null，这之后我们通过contains进行判断，此时如果有线程并发写入了一条value为null的值，那么contains的结果就为true。这样就会与真实的情况不一致了，这就是二义性。
